gstaichi.lang.misc
==================

.. py:module:: gstaichi.lang.misc


Attributes
----------

.. autoapisummary::

   gstaichi.lang.misc.i
   gstaichi.lang.misc.j
   gstaichi.lang.misc.k
   gstaichi.lang.misc.l
   gstaichi.lang.misc.ij
   gstaichi.lang.misc.ik
   gstaichi.lang.misc.il
   gstaichi.lang.misc.jk
   gstaichi.lang.misc.jl
   gstaichi.lang.misc.kl
   gstaichi.lang.misc.ijk
   gstaichi.lang.misc.ijl
   gstaichi.lang.misc.ikl
   gstaichi.lang.misc.jkl
   gstaichi.lang.misc.ijkl
   gstaichi.lang.misc.x86_64
   gstaichi.lang.misc.x64
   gstaichi.lang.misc.arm64
   gstaichi.lang.misc.cuda
   gstaichi.lang.misc.amdgpu
   gstaichi.lang.misc.metal
   gstaichi.lang.misc.vulkan
   gstaichi.lang.misc.gpu
   gstaichi.lang.misc.cpu
   gstaichi.lang.misc.extension


Functions
---------

.. autoapisummary::

   gstaichi.lang.misc.reset
   gstaichi.lang.misc.init
   gstaichi.lang.misc.no_activate
   gstaichi.lang.misc.block_local
   gstaichi.lang.misc.mesh_local
   gstaichi.lang.misc.cache_read_only
   gstaichi.lang.misc.assume_in_range
   gstaichi.lang.misc.loop_config
   gstaichi.lang.misc.global_thread_idx
   gstaichi.lang.misc.mesh_patch_idx


Module Contents
---------------

.. py:data:: i

   Axis 0. For multi-dimensional arrays it's the direction downward the rows.
   For a 1d array it's the direction along this array.

.. py:data:: j

   Axis 1. For multi-dimensional arrays it's the direction across the columns.

.. py:data:: k

   Axis 2. For arrays of dimension `d` >= 3, view each cell as an array of
   lower dimension d-2, it's the first axis of this cell.

.. py:data:: l

   Axis 3. For arrays of dimension `d` >= 4, view each cell as an array of
   lower dimension d-2, it's the second axis of this cell.

.. py:data:: ij

   Axes (0, 1).

.. py:data:: ik

   Axes (0, 2).

.. py:data:: il

   Axes (0, 3).

.. py:data:: jk

   Axes (1, 2).

.. py:data:: jl

   Axes (1, 3).

.. py:data:: kl

   Axes (2, 3).

.. py:data:: ijk

   Axes (0, 1, 2).

.. py:data:: ijl

   Axes (0, 1, 3).

.. py:data:: ikl

   Axes (0, 2, 3).

.. py:data:: jkl

   Axes (1, 2, 3).

.. py:data:: ijkl

   Axes (0, 1, 2, 3).

.. py:data:: x86_64

   The x64 CPU backend.

.. py:data:: x64

   The X64 CPU backend.

.. py:data:: arm64

   The ARM CPU backend.

.. py:data:: cuda

   The CUDA backend.

.. py:data:: amdgpu

   The AMDGPU backend.

.. py:data:: metal

   The Apple Metal backend.

.. py:data:: vulkan

   The Vulkan backend.

.. py:data:: gpu

   A list of GPU backends supported on the current system.
   Currently contains 'cuda', 'metal', 'vulkan', 'amdgpu'.

   When this is used, GsTaichi automatically picks the matching GPU backend. If no
   GPU is detected, GsTaichi falls back to the CPU backend.

.. py:data:: cpu

   A list of CPU backends supported on the current system.
   Currently contains 'x64', 'x86_64', 'arm64'.

   When this is used, GsTaichi automatically picks the matching CPU backend.

.. py:data:: extension

   An instance of GsTaichi extension.

   The list of currently available extensions is ['sparse', 'quant',     'mesh', 'quant_basic', 'data64', 'adstack', 'bls', 'assertion',         'extfunc'].

.. py:function:: reset()

   Resets GsTaichi to its initial state.
   This will destroy all the allocated fields and kernels, and restore
   the runtime to its default configuration.

   Example::

       >>> a = ti.field(ti.i32, shape=())
       >>> a[None] = 1
       >>> print("before reset: ", a)
       before rest: 1
       >>>
       >>> ti.reset()
       >>> print("after reset: ", a)
       # will raise error because a is unavailable after reset.


.. py:function:: init(arch=None, default_fp=None, default_ip=None, _test_mode: bool = False, enable_fallback: bool = True, require_version: str | None = None, print_non_pure: bool = False, src_ll_cache: bool = True, **kwargs)

   Initializes the GsTaichi runtime.

   This should always be the entry point of your GsTaichi program. Most
   importantly, it sets the backend used throughout the program.

   :param arch: Backend to use. This is usually :const:`~gstaichi.lang.cpu` or :const:`~gstaichi.lang.gpu`.
   :param default_fp: Default floating-point type.
   :type default_fp: Optional[type]
   :param default_ip: Default integral type.
   :type default_ip: Optional[type]
   :param require_version: A version string.
   :param print_non_pure: Print the names of kernels, at the time they are executed, which are not annotated with
                          @ti.pure
   :param src_ll_cache: enable SRC-LL-CACHE, which will accelerate loading from cache, across all architectures,
                        for pure kernels (i.e. kernels declared as @ti.pure)
   :param \*\*kwargs: GsTaichi provides highly customizable compilation through
                      ``kwargs``, which allows for fine grained control of GsTaichi compiler
                      behavior. Below we list some of the most frequently used ones. For a
                      complete list, please check out
                      https://github.com/taichi-dev/gstaichi/blob/master/gstaichi/program/compile_config.h.

                      * ``cpu_max_num_threads`` (int): Sets the number of threads used by the CPU thread pool.
                      * ``debug`` (bool): Enables the debug mode, under which GsTaichi does a few more things like boundary checks.
                      * ``print_ir`` (bool): Prints the CHI IR of the GsTaichi kernels.
                      *``offline_cache`` (bool): Enables offline cache of the compiled kernels. Default to True. When this is enabled GsTaichi will cache compiled kernel on your local disk to accelerate future calls.
                      *``random_seed`` (int): Sets the seed of the random generator. The default is 0.
                      *``debug_dump_path`` (str): used as the base path for TI_DUMP_KERNEL_CHECKSUMS and similar


.. py:function:: no_activate(*args)

   Deactivates a SNode pointer.


.. py:function:: block_local(*args)

   Hints GsTaichi to cache the fields and to enable the BLS optimization.

   Please visit https://docs.taichi-lang.org/docs/performance
   for how BLS is used.

   :param \*args: A list of sparse GsTaichi fields.
   :type \*args: List[Field]


.. py:function:: mesh_local(*args)

   Hints the compiler to cache the mesh attributes
   and to enable the mesh BLS optimization,
   only available for backends supporting `ti.extension.mesh` and to use with mesh-for loop.

   Related to https://github.com/taichi-dev/gstaichi/issues/3608

   :param \*args: A list of mesh attributes or fields accessed as attributes.
   :type \*args: List[Attribute]

   Examples::

       # instantiate model
       mesh_builder = ti.Mesh.tri()
       mesh_builder.verts.place({
           'x' : ti.f32,
           'y' : ti.f32
       })
       model = mesh_builder.build(meta)

       @ti.kernel
       def foo():
           # hint the compiler to cache mesh vertex attribute `x` and `y`.
           ti.mesh_local(model.verts.x, model.verts.y)
           for v0 in model.verts: # mesh-for loop
               for v1 in v0.verts:
                   v0.x += v1.y


.. py:function:: cache_read_only(*args)

.. py:function:: assume_in_range(val, base, low, high)

   Hints the compiler that a value is between a specified range,
   for the compiler to perform scatchpad optimization, and return the
   value untouched.

   The assumed range is `[base + low, base + high)`.

   :param val: The input value.
   :type val: Number
   :param base: The base point for the range interval.
   :type base: Number
   :param low: The lower offset relative to `base` (included).
   :type low: Number
   :param high: The higher offset relative to `base` (excluded).
   :type high: Number

   :returns: Return the input `value` untouched.

   Example::

       >>> # hint the compiler that x is in range [8, 12).
       >>> x = ti.assume_in_range(x, 10, -2, 2)
       >>> x
       10


.. py:function:: loop_config(*, block_dim=None, serialize=False, parallelize=None, block_dim_adaptive=True, bit_vectorize=False)

   Sets directives for the next loop

   :param block_dim: The number of threads in a block on GPU
   :type block_dim: int
   :param serialize: Whether to let the for loop execute serially, `serialize=True` equals to `parallelize=1`
   :type serialize: bool
   :param parallelize: The number of threads to use on CPU
   :type parallelize: int
   :param block_dim_adaptive: Whether to allow backends set block_dim adaptively, enabled by default
   :type block_dim_adaptive: bool
   :param bit_vectorize: Whether to enable bit vectorization of struct fors on quant_arrays.
   :type bit_vectorize: bool

   Examples::

       @ti.kernel
       def break_in_serial_for() -> ti.i32:
           a = 0
           ti.loop_config(serialize=True)
           for i in range(100):  # This loop runs serially
               a += i
               if i == 10:
                   break
           return a

       break_in_serial_for()  # returns 55

       n = 128
       val = ti.field(ti.i32, shape=n)
       @ti.kernel
       def fill():
           ti.loop_config(parallelize=8, block_dim=16)
           # If the kernel is run on the CPU backend, 8 threads will be used to run it
           # If the kernel is run on the CUDA backend, each block will have 16 threads.
           for i in range(n):
               val[i] = i

       u1 = ti.types.quant.int(bits=1, signed=False)
       x = ti.field(dtype=u1)
       y = ti.field(dtype=u1)
       cell = ti.root.dense(ti.ij, (128, 4))
       cell.quant_array(ti.j, 32).place(x)
       cell.quant_array(ti.j, 32).place(y)
       @ti.kernel
       def copy():
           ti.loop_config(bit_vectorize=True)
           # 32 bits, instead of 1 bit, will be copied at a time
           for i, j in x:
               y[i, j] = x[i, j]


.. py:function:: global_thread_idx()

   Returns the global thread id of this running thread,
   only available for cpu and cuda backends.

   For cpu backends this is equal to the cpu thread id,
   For cuda backends this is equal to `block_id * block_dim + thread_id`.

   Example::

       >>> f = ti.field(ti.f32, shape=(16, 16))
       >>> @ti.kernel
       >>> def test():
       >>>     for i in ti.grouped(f):
       >>>         print(ti.global_thread_idx())
       >>>
       test()


.. py:function:: mesh_patch_idx()

   Returns the internal mesh patch id of this running thread,
   only available for backends supporting `ti.extension.mesh` and to use within mesh-for loop.

   Related to https://github.com/taichi-dev/gstaichi/issues/3608


